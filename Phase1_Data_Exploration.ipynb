{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5af6d70",
   "metadata": {},
   "source": [
    "# Phase 1 — Problem Understanding & Data Exploration\n",
    "**Project:** Student Performance Predictor — Group 2  \n",
    "**Date:** 2025-10-16\n",
    "\n",
    "This notebook fulfills SWE485 Phase 1 requirements: Dataset Goal & Source, General Info, Summary & Visualization, and Preprocessing (with justifications)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1775a",
   "metadata": {},
   "source": [
    "## 1) Dataset Goal & Source\n",
    "**Goal:** Predict pass/fail or final grade (`G3`) and derive simple study advice.\n",
    "\n",
    "**Source:**\n",
    "Kaggle dataset typically provides two files:\n",
    "- `student-mat.csv` (Math course)\n",
    "- `student-por.csv` (Portuguese course)\n",
    "\n",
    "You may also provide a single unified file named `student_performance.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684d25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Setup & Imports ============\n",
    "import os, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = \"./Dataset\"\n",
    "CANDIDATES = [\n",
    "    os.path.join(DATA_DIR, \"student_performance.csv\"),\n",
    "    os.path.join(DATA_DIR, \"student-mat.csv\"),\n",
    "    os.path.join(DATA_DIR, \"student-por.csv\"),\n",
    "]\n",
    "\n",
    "existing = [p for p in CANDIDATES if os.path.exists(p)]\n",
    "assert os.path.exists(DATA_DIR), \"Please create a /Dataset folder next to this notebook.\"\n",
    "assert len(existing) > 0, \"Place a CSV in /Dataset: student_performance.csv OR student-mat.csv / student-por.csv\"\n",
    "print(\"Found files:\", existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8baad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Load Data ============\n",
    "def load_student_data(paths):\n",
    "    dfs = []\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                df = pd.read_csv(p, sep=',')  # most versions are comma-separated\n",
    "            except Exception:\n",
    "                df = pd.read_csv(p, sep=';')  # fallback for semicolon-separated\n",
    "            df['__source__'] = os.path.basename(p)\n",
    "            dfs.append(df)\n",
    "    if not dfs:\n",
    "        raise FileNotFoundError(\"No valid CSV found.\")\n",
    "    # Align common columns if multiple files exist, then concatenate\n",
    "    base_cols = set(dfs[0].columns)\n",
    "    for i in range(1, len(dfs)):\n",
    "        base_cols = base_cols.intersection(set(dfs[i].columns))\n",
    "    dfs = [d[list(sorted(base_cols))].copy() for d in dfs]\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df = load_student_data(existing)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28689fd3",
   "metadata": {},
   "source": [
    "## 2) General Information\n",
    "- Observations & features\n",
    "- Data types\n",
    "- Target variable: `G3` (final grade) or derived `passed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139848f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows, Cols:\", df.shape)\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\nColumn Dtypes:\")\n",
    "print(df.dtypes)\n",
    "df.sample(min(5, len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b82a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Set Target Column ============\n",
    "TARGET = None\n",
    "for cand in ['G3', 'final_grade', 'Grade', 'grade']:\n",
    "    if cand in df.columns:\n",
    "        TARGET = cand\n",
    "        break\n",
    "\n",
    "if TARGET is None:\n",
    "    # Create a binary 'passed' if a numeric grade exists\n",
    "    for gcol in ['G3', 'final_grade', 'Grade', 'grade']:\n",
    "        if gcol in df.columns and pd.api.types.is_numeric_dtype(df[gcol]):\n",
    "            median_val = df[gcol].median()\n",
    "            df['passed'] = (df[gcol] >= median_val).astype(int)\n",
    "            TARGET = 'passed'\n",
    "            print(f\"Derived TARGET 'passed' from {gcol} using median={median_val:.2f}\")\n",
    "            break\n",
    "\n",
    "if TARGET is None:\n",
    "    raise ValueError(\"Please set TARGET manually to an existing column.\")\n",
    "\n",
    "print(\"TARGET =\", TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69148ded",
   "metadata": {},
   "source": [
    "## 3) Summary & Visualization\n",
    "- Statistical summaries\n",
    "- Missing values\n",
    "- Distributions and (if applicable) class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237fa9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df.describe(include='all').transpose().fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49b1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values overview\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "missing[missing>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f6680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributions for numeric columns\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "for col in num_cols[:10]:  # cap to first 10\n",
    "    plt.figure()\n",
    "    df[col].hist(bins=30)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col); plt.ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "# Class balance if we have a categorical/binary target\n",
    "if TARGET in df.columns and not pd.api.types.is_numeric_dtype(df[TARGET]):\n",
    "    plt.figure()\n",
    "    df[TARGET].value_counts().plot(kind='bar')\n",
    "    plt.title(f\"Class Balance: {TARGET}\")\n",
    "    plt.xlabel(\"Class\"); plt.ylabel(\"count\")\n",
    "    plt.show()\n",
    "elif TARGET == 'passed':\n",
    "    plt.figure()\n",
    "    df['passed'].value_counts().plot(kind='bar')\n",
    "    plt.title(\"Class Balance: passed\")\n",
    "    plt.xlabel(\"Class\"); plt.ylabel(\"count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b958089d",
   "metadata": {},
   "source": [
    "## 4) Preprocessing Techniques (with justifications)\n",
    "Document what you changed and why:\n",
    "- Handle missing values: drop/fill/impute.\n",
    "- Encode categoricals: One-Hot Encoding.\n",
    "- Scale numeric features if needed.\n",
    "- Remove/transform outliers if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d5ab65",
   "metadata": {},
   "outputs": [],
   "source": [
  # ============ Preprocessing Techniques (with justifications) ============

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
import numpy as np

# --- Check that TARGET exists and is valid for the new dataset ---
if 'math_score' in df.columns and 'reading_score' in df.columns and 'writing_score' in df.columns:
    if 'avg_score' not in df.columns:
        df['avg_score'] = df[['math_score','reading_score','writing_score']].mean(axis=1)
    if 'TARGET' not in globals() or TARGET not in df.columns:
        df['passed'] = (df['avg_score'] >= 70).astype(int)
        TARGET = 'passed'

assert TARGET in df.columns, "TARGET column is not set. Make sure TARGET exists (e.g., 'passed' or 'avg_score')."

# --- Split features and target ---
X = df.drop(columns=[TARGET]).copy()
y = df[TARGET].copy()

# Avoid data leakage: if the target is 'passed' (derived from avg_score), remove avg_score from X
if TARGET == 'passed' and 'avg_score' in X.columns:
    X.drop(columns=['avg_score'], inplace=True)

# --- Identify numeric and categorical columns ---
num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()

# --- Define preprocessing pipelines ---
num_pipe = Pipeline([
    ("imp", SimpleImputer(strategy="median")),   # Impute missing numeric values using the median
])

cat_pipe = Pipeline([
    ("imp", SimpleImputer(strategy="most_frequent")),  # Impute missing categorical values with the most frequent
    ("ohe", OneHotEncoder(handle_unknown="ignore"))    # Encode categorical features using One-Hot Encoding
])

# --- Combine both numeric and categorical transformations ---
preprocessor = ColumnTransformer([
    ("num", num_pipe, num_cols),
    ("cat", cat_pipe, cat_cols),
], remainder="drop")

# --- Apply transformations ---
X_ready = preprocessor.fit_transform(X)

print("Processed shape:", X_ready.shape)
print("Num features:", len(num_cols), "| Cat features:", len(cat_cols))

#  Notes (write these below the cell in Markdown):
# - Missing values: median for numeric features, most_frequent for categorical.
# - Encoding: One-Hot encoding for categorical variables.
# - Leakage prevention: removed avg_score from X when the target is 'passed'.
# - Scaling: to be added in Phase 2 if models like SVM or Logistic Regression are used.
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb701a",
   "metadata": {},
   "source": [
    "## 5) Notes & Next Steps\n",
    "- Summarize key findings.\n",
    "- List data issues or open questions.\n",
    "- Plan candidate models for Phase 2 (e.g., Decision Tree, SVM)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
